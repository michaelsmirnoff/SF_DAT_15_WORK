{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Linear Regression\n",
    "# *Adapted from Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "# # Part 1: Introduction\n",
    "# - **Classification problem:** supervised learning problem with a categorical response\n",
    "# - **Regression problem**: supervised learning problem with a continuous response\n",
    "# - **Linear regression:** machine learning model that can be used for regression problems\n",
    "\n",
    "# Why are we learning linear regression?\n",
    "# - widely used\n",
    "# - runs fast\n",
    "# - easy to use (no tuning is required)\n",
    "# - highly interpretable\n",
    "# - basis for many other methods\n",
    "\n",
    "# Lesson goals:\n",
    "# - Conceptual understanding of linear regression and how it \"works\"\n",
    "# - Familiarity with key terminology\n",
    "# - Ability to apply linear regression to a machine learning problem using scikit-learn\n",
    "# - Ability to interpret model coefficients\n",
    "# - Familiarity with different approaches for feature selection\n",
    "# - Understanding of three different evaluation metrics for regression\n",
    "# - Understanding of linear regression's strengths and weaknesses\n",
    "\n",
    "# ## Libraries\n",
    "# - [Statsmodels](http://statsmodels.sourceforge.net/): \"statistics in Python\"\n",
    "#     - robust functionality for linear modeling\n",
    "#     - useful for teaching purposes\n",
    "#     - will not be used in the course outside of this lesson\n",
    "# - [scikit-learn](http://scikit-learn.org/stable/): \"machine learning in Python\"\n",
    "#     - significantly more functionality for general purpose machine learning\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ## Reading the advertising data\n",
    "\n",
    "# read data into a DataFrame\n",
    "data = pd.read_csv('http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv', index_col=0)\n",
    "data.head()\n",
    "\n",
    "# What are the observations?\n",
    "# - Each observation represents **one market** (200 markets in the dataset)\n",
    "\n",
    "# What are the features?\n",
    "# - **TV:** advertising dollars spent on TV for a single product (in thousands of dollars)\n",
    "# - **Radio:** advertising dollars spent on Radio\n",
    "# - **Newspaper:** advertising dollars spent on Newspaper\n",
    "\n",
    "# What is the response?\n",
    "# - **Sales:** sales of a single product in a given market (in thousands of widgets)\n",
    "\n",
    "# ## Questions about the data\n",
    "# You are asked by the company: On the basis of this data, how should we spend our advertising money in the future?\n",
    "# You come up with more specific questions:\n",
    "# 1. Is there a relationship between ads and sales?\n",
    "# 2. How strong is that relationship?\n",
    "# 3. Which ad types contribute to sales?\n",
    "# 4. What is the effect of each ad type of sales?\n",
    "# 5. Given ad spending in a particular market, can sales be predicted?\n",
    "\n",
    "# ## Visualizing the data\n",
    "\n",
    "# Use a **scatter plot** to visualize the relationship between the features and the response.\n",
    "\n",
    "# scatter plot in Seaborn\n",
    "sns.pairplot(data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=4.5, aspect=0.7)\n",
    "\n",
    "# include a \"regression line\"\n",
    "sns.pairplot(data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=4.5, aspect=0.7, kind='reg')\n",
    "\n",
    "# scatter plot in Pandas\n",
    "fig, axs = plt.subplots(1, 3, sharey=True)\n",
    "data.plot(kind='scatter', x='TV', y='Sales', ax=axs[0], figsize=(16, 6))\n",
    "data.plot(kind='scatter', x='Radio', y='Sales', ax=axs[1])\n",
    "data.plot(kind='scatter', x='Newspaper', y='Sales', ax=axs[2])\n",
    "\n",
    "# Use a **scatter matrix** to visualize the relationship between all numerical variables.\n",
    "\n",
    "# scatter matrix in Seaborn\n",
    "sns.pairplot(data)\n",
    "\n",
    "# scatter matrix in Pandas\n",
    "pd.scatter_matrix(data, figsize=(12, 10))\n",
    "\n",
    "# Use a **correlation matrix** to visualize the correlation between all numerical variables.\n",
    "\n",
    "# compute correlation matrix\n",
    "data.corr()\n",
    "\n",
    "# display correlation matrix in Seaborn using a heatmap\n",
    "sns.heatmap(data.corr())\n",
    "\n",
    "# # Part 2: Simple linear regression\n",
    "\n",
    "# Simple linear regression is an approach for predicting a **continuous response** using a **single feature**. It takes the following form:\n",
    "# $y = \\beta_0 + \\beta_1x$\n",
    "# - $y$ is the response\n",
    "# - $x$ is the feature\n",
    "# - $\\beta_0$ is the intercept\n",
    "# - $\\beta_1$ is the coefficient for x\n",
    "\n",
    "# $\\beta_0$ and $\\beta_1$ are called the **model coefficients**:\n",
    "\n",
    "# - We must \"learn\" the values of these coefficients to create our model.\n",
    "# - And once we've learned these coefficients, we can use the model to predict Sales.\n",
    "\n",
    "# ## Estimating (\"learning\") model coefficients\n",
    "# - Coefficients are estimated during the model fitting process using the **least squares criterion**.\n",
    "# - We are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\").\n",
    "\n",
    "# In this diagram:\n",
    "# - The black dots are the **observed values** of x and y.\n",
    "# - The blue line is our **least squares line**.\n",
    "# - The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "# How do the model coefficients relate to the least squares line?\n",
    "# - $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "# - $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "# Let's estimate the model coefficients for the advertising data:\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# create a fitted model\n",
    "lm = smf.ols(formula='Sales ~ TV', data=data).fit()\n",
    "\n",
    "# print the coefficients\n",
    "lm.params\n",
    "\n",
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['TV']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_\n",
    "\n",
    "# ## Interpreting model coefficients\n",
    "\n",
    "# How do we interpret the TV coefficient ($\\beta_1$)?\n",
    "# - A \"unit\" increase in TV ad spending is **associated with** a 0.0475 \"unit\" increase in Sales.\n",
    "# - Meaning: An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.5 widgets.\n",
    "# - This is not a statement of **causation**.\n",
    "\n",
    "# If an increase in TV ad spending was associated with a **decrease** in sales, $\\beta_1$ would be **negative**.\n",
    "\n",
    "# ## Using the model for prediction\n",
    "\n",
    "# Let's say that there was a new market where the TV advertising spend was **$50,000**. What would we predict for the Sales in that market?\n",
    "# $$y = \\beta_0 + \\beta_1x$$\n",
    "# $$y = 7.0326 + 0.0475 \\times 50$$\n",
    "\n",
    "# manually calculate the prediction\n",
    "7.0326 + 0.0475*50\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# you have to create a DataFrame since the Statsmodels formula interface expects it\n",
    "X_new = pd.DataFrame({'TV': [50]})\n",
    "\n",
    "# predict for a new observation\n",
    "lm.predict(X_new)\n",
    "\n",
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# predict for a new observation\n",
    "linreg.predict(50)\n",
    "\n",
    "# Thus, we would predict Sales of **9,409 widgets** in that market.\n",
    "\n",
    "# ## Does the scale of the features matter?\n",
    "\n",
    "# Let's say that TV was measured in dollars, rather than thousands of dollars. How would that affect the model?\n",
    "\n",
    "data['TV_dollars'] = data.TV * 1000\n",
    "data.head()\n",
    "\n",
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['TV_dollars']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_\n",
    "\n",
    "# How do we interpret the TV_dollars coefficient ($\\beta_1$)?\n",
    "# - A \"unit\" increase in TV ad spending is **associated with** a 0.0000475 \"unit\" increase in Sales.\n",
    "# - Meaning: An additional dollar spent on TV ads is **associated with** an increase in sales of 0.0475 widgets.\n",
    "# - Meaning: An additional $1,000 spent on TV ads is **associated with** an increase in sales of 47.5 widgets.\n",
    "\n",
    "# predict for a new observation\n",
    "linreg.predict(50000)\n",
    "\n",
    "# The scale of the features is **irrelevant** for linear regression models, since it will only affect the scale of the coefficients, and we simply change our interpretation of the coefficients.\n",
    "\n",
    "# # Part 3: A deeper understanding\n",
    "\n",
    "# ## Bias and variance\n",
    "\n",
    "# Linear regression is a low variance/high bias model:\n",
    "# - **Low variance:** Under repeated sampling from the underlying population, the line will stay roughly in the same place\n",
    "# - **High bias:** The line will rarely fit the data well\n",
    "\n",
    "# A closely related concept is **confidence intervals**.\n",
    "\n",
    "# ## Confidence intervals\n",
    "\n",
    "# Statsmodels calculates 95% confidence intervals for our model coefficients, which are interpreted as follows: If the population from which this sample was drawn was **sampled 100 times**, approximately **95 of those confidence intervals** would contain the \"true\" coefficient.\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# print the confidence intervals for the model coefficients\n",
    "lm.conf_int()\n",
    "\n",
    "# - We only have a **single sample of data**, and not the **entire population of data**.\n",
    "# - The \"true\" coefficient is either within this interval or it isn't, but there's no way to actually know.\n",
    "# - We estimate the coefficient with the data we do have, and we show uncertainty about that estimate by giving a range that the coefficient is **probably** within.\n",
    "# - From Quora: [What is a confidence interval in layman's terms?](http://www.quora.com/What-is-a-confidence-interval-in-laymans-terms/answer/Michael-Hochster)\n",
    "\n",
    "# Note: 95% confidence intervals are just a convention. You can create 90% confidence intervals (which \n",
    "#will be more narrow), 99% confidence intervals (which will be wider), or whatever intervals you like.\n",
    "\n",
    "# A closely related concept is **hypothesis testing**.\n",
    "\n",
    "# ## Hypothesis testing and p-values\n",
    "\n",
    "# General process for hypothesis testing:\n",
    "# - You start with a **null hypothesis** and an **alternative hypothesis** (that is opposite the null).\n",
    "# - You check whether the data supports **rejecting the null hypothesis** or **failing to reject the null hypothesis**.\n",
    "\n",
    "# For model coefficients, here is the conventional hypothesis test:\n",
    "# - **null hypothesis:** There is no relationship between TV ads and Sales (and thus $\\beta_1$ equals zero)\n",
    "# - **alternative hypothesis:** There is a relationship between TV ads and Sales (and thus $\\beta_1$ is not equal to zero)\n",
    "\n",
    "# How do we test this hypothesis?\n",
    "# - The **p-value** is the probability that the relationship we are observing is occurring purely by chance.\n",
    "# - If the 95% confidence interval for a coefficient **does not include zero**, the p-value will be **less than 0.05**, and we will reject the null (and thus believe the alternative).\n",
    "# - If the 95% confidence interval **includes zero**, the p-value will be **greater than 0.05**, and we will fail to reject the null.\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# print the p-values for the model coefficients\n",
    "lm.pvalues\n",
    "\n",
    "# Thus, a p-value less than 0.05 is one way to decide whether there is **likely** a relationship between the feature and the response. In this case, the p-value for TV is far less than 0.05, and so we **believe** that there is a relationship between TV ads and Sales.\n",
    "\n",
    "# Note that we generally ignore the p-value for the intercept.\n",
    "\n",
    "# ## How well does the model fit the data?\n",
    "\n",
    "# R-squared:\n",
    "# - A common way to evaluate the overall fit of a linear model\n",
    "# - Defined as the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model\n",
    "# - Also defined as the reduction in error over the **null model**, which is the model that simply predicts the mean of the observed response\n",
    "# - Between 0 and 1, and higher is better\n",
    "\n",
    "# Here's an example of what R-squared \"looks like\":\n",
    "\n",
    "# Let's calculate the R-squared value for our simple linear model:\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# print the R-squared value for the model\n",
    "lm.rsquared\n",
    "\n",
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# calculate the R-squared value for the model\n",
    "y_pred = linreg.predict(X)\n",
    "metrics.r2_score(y, y_pred)\n",
    "\n",
    "# - The threshold for a **\"good\" R-squared value** is highly dependent on the particular domain.\n",
    "# - R-squared is more useful as a tool for **comparing models**.\n",
    "\n",
    "# # Part 4: Multiple Linear Regression\n",
    "\n",
    "# Simple linear regression can easily be extended to include multiple features, which is called **multiple linear regression**:\n",
    "# $y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "# Each $x$ represents a different feature, and each feature has its own coefficient:\n",
    "# $y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times Radio + \\beta_3 \\times Newspaper$\n",
    "\n",
    "### SCIKIT-LEARN ###\n",
    "\n",
    "# create X and y\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "\n",
    "# instantiate and fit\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print linreg.intercept_\n",
    "print linreg.coef_\n",
    "\n",
    "# pair the feature names with the coefficients\n",
    "zip(feature_cols, linreg.coef_)\n",
    "\n",
    "# For a given amount of Radio and Newspaper spending, an increase of $1000 in **TV** spending is associated with an **increase in Sales of 45.8 widgets**.\n",
    "# For a given amount of TV and Newspaper spending, an increase of $1000 in **Radio** spending is associated with an **increase in Sales of 188.5 widgets**.\n",
    "# For a given amount of TV and Radio spending, an increase of $1000 in **Newspaper** spending is associated with an **decrease in Sales of 1.0 widgets**. How could that be?\n",
    "\n",
    "# ## Feature selection\n",
    "\n",
    "# How do I decide **which features to include** in a linear model?\n",
    "\n",
    "# ### Using p-values\n",
    "\n",
    "# We could try a model with all features, and only keep features in the model if they have **small p-values**:\n",
    "\n",
    "### STATSMODELS ###\n",
    "\n",
    "# create a fitted model with all three features\n",
    "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\n",
    "\n",
    "# print the p-values for the model coefficients\n",
    "print lm.pvalues\n",
    "\n",
    "# This indicates we would reject the null hypothesis for **TV and Radio** (that there is no association between those features and Sales), and fail to reject the null hypothesis for **Newspaper**. Thus, we would keep TV and Radio in the model.\n",
    "\n",
    "# However, this approach has **drawbacks**:\n",
    "# - Linear models rely upon a lot of **assumptions** (such as the features being independent), and if those assumptions are violated (which they usually are), p-values are less reliable.\n",
    "# - Using a p-value cutoff of 0.05 means that if you add 100 features to a model that are **pure noise**, 5 of them (on average) will still be counted as significant.\n",
    "\n",
    "# ### Using R-squared\n",
    "# We could try models with different sets of features, and **compare their R-squared values**:\n",
    "\n",
    "# R-squared value for the model with two features\n",
    "lm = smf.ols(formula='Sales ~ TV + Radio', data=data).fit()\n",
    "lm.rsquared\n",
    "\n",
    "# R-squared value for the model with three features\n",
    "lm = smf.ols(formula='Sales ~ TV + Radio + Newspaper', data=data).fit()\n",
    "lm.rsquared\n",
    "\n",
    "# This would seem to indicate that the best model includes **all three features**. Is that right?\n",
    "# - R-squared will always increase as you add more features to the model, even if they are **unrelated** to the response.\n",
    "# - As such, using R-squared as a model evaluation metric can lead to **overfitting**.\n",
    "# - **Adjusted R-squared** is an alternative that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html).\n",
    "\n",
    "# As well, R-squared depends on the same assumptions as p-values, and it's less reliable if those assumptions are violated.\n",
    "\n",
    "# ### Using train/test split (or cross-validation)\n",
    "\n",
    "# A better approach to feature selection!\n",
    "# - They attempt to directly estimate how well your model will **generalize** to out-of-sample data.\n",
    "# - They rely on **fewer assumptions** that linear regression.\n",
    "# - They can easily be applied to **any model**, not just linear models.\n",
    "\n",
    "# ## Evaluation metrics for regression problems\n",
    "\n",
    "# Evaluation metrics for classification problems, such as **accuracy**, are not useful for regression problems. We need evaluation metrics designed for comparing **continuous values**.\n",
    "# Let's create some example numeric predictions, and calculate three common evaluation metrics for regression problems:\n",
    "\n",
    "# define true and predicted response values\n",
    "y_true = [100, 50, 30, 20]\n",
    "y_pred = [90, 50, 50, 30]\n",
    "\n",
    "# **Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "# $$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "print metrics.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# **Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "# $$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "print metrics.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# **Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n",
    "# $$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "print np.sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Comparing these metrics:\n",
    "# - **MAE** is the easiest to understand, because it's the average error.\n",
    "# - **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n",
    "# - **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n",
    "\n",
    "# All of these are **loss functions**, because we want to minimize them.\n",
    "\n",
    "# Here's an additional example, to demonstrate how MSE/RMSE punish larger errors:\n",
    "\n",
    "# same true values as above\n",
    "y_true = [100, 50, 30, 20]\n",
    "\n",
    "# new set of predicted values\n",
    "y_pred = [60, 50, 30, 20]\n",
    "\n",
    "# MAE is the same as before\n",
    "print metrics.mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# RMSE is larger than before\n",
    "\n",
    "# ## Using train/test split for feature selection\n",
    "# Let's use train/test split with RMSE to decide whether Newspaper should be kept in the model:\n",
    "\n",
    "# define a function that accepts X and y and computes testing RMSE\n",
    "def train_test_rmse(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    linreg = LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_test)\n",
    "    return np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# include Newspaper\n",
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "train_test_rmse(X, y)\n",
    "\n",
    "# exclude Newspaper\n",
    "feature_cols = ['TV', 'Radio']\n",
    "X = data[feature_cols]\n",
    "train_test_rmse(X, y)\n",
    "\n",
    "\n",
    "# ## Comparing linear regression with other models\n",
    "\n",
    "# Advantages of linear regression:\n",
    "# - Simple to explain\n",
    "# - Highly interpretable\n",
    "# - Model training and prediction are fast\n",
    "# - No tuning is required (excluding regularization)\n",
    "# - Features don't need scaling\n",
    "# - Can perform well with a small number of observations\n",
    "\n",
    "# Disadvantages of linear regression:\n",
    "# - Presumes a linear relationship between the features and the response\n",
    "# - Performance is (generally) not competitive with the best supervised learning methods due to high bias\n",
    "# - Sensitive to irrelevant features\n",
    "# - Can't automatically learn feature interactions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
