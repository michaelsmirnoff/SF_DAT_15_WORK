# My Question

## Background
We keep a close eye on the [American Customer Satisfaction Index](http://www.theacsi.org/the-american-customer-satisfaction-index) score for our website, [SEC.gov](http://www.sec.gov).  The score is generated by a user survey.  When we review the data, we look at average responses across all users.  This has confirmed the obvious: People really like our content.  The navigation is frustrating.  

We could learn so much more from this data!  The SEC serves multiple different audience groups.  A novice investor may be frustrated by a complex search tool, which may be perfect for a professional research analyst.  A lawyer may prefer an index by case number, but a journalist may be frustrated that they can't navigate by topic.  

By analyzing the survey results, we may be able to identify specific improvements that will improve the experience for specific groups of users.


## My data
I was able to construct a csv file of all survey responses from the two years it has been live.  There are **9,678 survey responses**.  Each survey response includes:

* **21 multiple choice questions**  - *most of these questions are required and have complete data.  Three questions are optional and have between 2,552 and 7,528 responses.*


* **10 free text questions** -  *these questions are optional; the response rate varies from less than 1% to nearly 50%*.


* **19 system generated fields** - *17 of the fields have values for more than 95% of responses. Two of these fields only have values in 40% of responses.* 


* **14 system generated fields for mobile responses** - *this only applies to 5 of the survey responses.*

I've uploaded a [small sample of the data](/laura/survey_data_example.csv).

## Question(s)
This is probably **way too broad** but it's a start.  I'm still sorting through multiple questions, but I believe that they can help me explore the data and refine my project:

### What correlates with satisfaction?

I'd like to examine the correlation between satisfaction with our website and the following factors:

* user's role (i.e. educator, financial professional, journalist)
* what section of the website they were on

And maybe these:

* [EDGAR](http://www.sec.gov/edgar/searchedgar/companysearch.html) search users vs. non-EDGAR users *(there are a few different ways to determine this, which deserves a close look)*
* frequency of visits to our website
* users referred through search engine vs. a link on another website
* number of pageviews before taking the survey
* operating system and browser
* time of day (why not?)

I could also see if user's role correlates with other factors.

### Can we cluster the free text responses into topics?

It would be very interesting to categorize the free text responses to these questions into themes or topics:

* If you could make one change/improvement to the SEC website, what would it be? ***4,826 responses***


* Please tell us specifically what were you unable to find or accomplish? ***2,066 responses***


* Please briefly explain your experience with the search tool? ***1,317 responses***

I could then see how the topic groups relate to user's role or section of the website! Fun!


## What's up with surveys?

Who has enough interest and time to take surveys?

Clearly 9,768 people did, but can we assume they are a representative sample?  Is there any research into this?