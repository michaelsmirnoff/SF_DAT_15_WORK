{
 "metadata": {
  "name": "",
  "signature": "sha256:4114b8e0c048547d041faad968b6cbcf5451957deca310102d39198bf674a292"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Selling Ideas: Examining the Relationships Between Startups with Patents and Their Ability to Raise Funds"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Introduction"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Innovation\" has been a prominent buzzword in the American lexicon for nearly a decade. Startups with new ideas and access to tremendous computing resources have made tremendous impacts on numerous industries.\n",
      "\n",
      "One traditional measure of innovation is the number of patents a firm or person has. While using this as a metric is not without controversy, it makes a level of intuitive sense even in the modern day. Many of the startups created in the past ten years or more are not capital-intensive concerns. Their main assets are their ideas. One method of protecting this intellectual property is to obtain a patent.\n",
      "\n",
      "Furthermore, given that new ideas are one of the primary differentiators between companies, investors judge where to put their money based, in part, on the nature and novelty of a firm's offerings. Patents may provide a window into what exactly investors see in particular startups.\n",
      "\n",
      "I will examine what, if any, correlations can be made between startups with patents and the amount of money those firms are able to raise. Patent data is obtained from the US Patent and Trademark Office's [Patent Full Grant Text bulk download](https://www.google.com/googlebooks/uspto-patents-grants-text.html) page hosted by Google. Startup funding data is gleaned via the application program interface (API) offered by the website [Crunchbase](https://www.crunchbase.com/).\n",
      "\n",
      "**Note:** Some files have been provided where noted, so the reader only has to run the code s/he deems necessary."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Acquisition"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To begin, I downloaded a batch of zipped patents from the patent repository. A zipped file of patents is released weekly, and the full texts of patents is available from 1976 to the present day. Files for patents granted in the year 2001 and later are in XML format; prior to that, the files are in TXT format. \n",
      "\n",
      "I acquired the patents from the week of January 3, 2012, to the week of October 7, 2014 (the last week available at the beginning of the project). My intention had been to download and analyze all of the patents since 2001, but each zipped file is between 400MB and 700MB and the unzipped XMLs from that time period take up approximately 50GB of space, so hard drive space limitations limited me to this timeframe.\n",
      "\n",
      "To download the files, I used the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# All modules\n",
      "\n",
      "%matplotlib inline\n",
      "import re\n",
      "import requests\n",
      "import urllib\n",
      "import zipfile\n",
      "import os\n",
      "import csv\n",
      "import sys\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "from bs4 import BeautifulSoup\n",
      "try:\n",
      "    import xml.etree.cElementTree as ET\n",
      "except ImportError:\n",
      "    import xml.etree.ElementTree as ET\n",
      "\n",
      "# Set options\n",
      "pd.set_option('html', False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Download the USPTO bulk download webpage \n",
      "r = requests.get(\"https://www.google.com/googlebooks/uspto-patents-grants-text.html\")\n",
      "\n",
      "# Set the regex pattern for the files.\n",
      "# The format of the filenames is 'YYYY/ipgYYMMDD.zip', e.g. 2014/ipg140318.zip. \n",
      "# Filenames from 2004 and before omit the initial 'i', but that ended up being irrelevant.\n",
      "# Regex pattern = \\d+/i?pg\\d+\\.zip\n",
      "re_tag = re.compile(\"http://storage\\.googleapis\\.com/patents/grant_full_text/\\d+/i?pg\\d+\\.zip\")\n",
      "\n",
      "# Create a list of the relevant urls\n",
      "zips = re.findall(re_tag, r.text)\n",
      "\n",
      "destinationPath = 'data/USPTO_zipfiles/'\n",
      "\n",
      "# Example: Download and extract the zipfiles \n",
      "for i in range(4):\n",
      "    filename =  zips[i][59:]\n",
      "    try:\n",
      "        urllib.urlretrieve(zips[i], filename)\n",
      "    except ValueError:\n",
      "        continue\n",
      "    sourceZip = zipfile.ZipFile(filename, 'r')\n",
      "    \n",
      "    # Extracts the zipfiles and place in the destination\n",
      "    for name in sourceZip.namelist():\n",
      "        sourceZip.extract(name, destinationPath)\n",
      "    sourceZip.close\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Parsing the XMLs proved to be the first hurdle. All of the python xml parsing modules I tried require the file to be 'wrapped' with some variation of `root` and `end` tags. The XMLs provided by USPTO do not have those. At best, the parser would read the first record, reach that record's `end` tag, and stop. So, I rewrote the XMLs by adding the root and end tags `<patents>` and\n",
      "`</patents>` to the beginning and end of each file and skipping the lines that interrupted the parsing."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xml_source_path = \"data/USPTO_zipfiles/\"\n",
      "xml_destination_path = \"data/formatted_xmls/\"\n",
      "\n",
      "xml_source_list = [xml_source_path + x for x in os.listdir(xml_source_path)[1:]]\n",
      "\n",
      "# Reformat xmls to work with XML parser\n",
      "for xml in xml_source_list:\n",
      "    text = '<patents>\\n'\n",
      "    with open(xml,'rU') as f:\n",
      "       for line in f.readlines():\n",
      "           if '<?xml' not in line and '<!DOCTYPE' not in line: \n",
      "               text += line\n",
      "    text += '</patents>\\n'\n",
      "    with open(xml_destination_path + xml[-13:], 'wb') as g:\n",
      "        g.write(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: '/data/USPTO_zipfiles/'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-6-4918d6fe8f34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxml_destination_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/data/formatted_xmls/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxml_source_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxml_source_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_source_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Reformat xmls to work with XML parser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/data/USPTO_zipfiles/'"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the XMLs now in serviceable condition, I opted to use the [ElementTree module](https://docs.python.org/2/library/xml.etree.elementtree.html) (specifically, the C implementation of ElementTree) for parsing. There is ample documentation on how to use this module, the C implementation is (apparently) faster than others implemented in pure Python, and ElementTree has better [XPATH](http://www.w3.org/TR/xpath/) support than other modules such as Beautiful Soup.\n",
      "\n",
      "At this time I was not certain what data would be relevant, so I opted for inclusivity. The elements of data collected were:\n",
      "1. The assigned patent number\n",
      "2. The invention title\n",
      "3. The [type of patent](http://www.uspto.gov/learning-and-resources/support-centers/electronic-business-center/kind-codes-included-uspto-patent) granted\n",
      "4. The date of the patent grant\n",
      "5. The type of patent applied for\n",
      "6. The date the application was made\n",
      "7. The [main US government classification](http://www.uspto.gov/web/patents/classification/selectnumwithtitle.htm) of the patent\n",
      "8. The number of references cited by the patent applicant and patent examiners\n",
      "9. The holder(s) of the patent\n",
      "10. The city, state, and country of the patent holder(s)\n",
      "11. The name(s) of the inventor(s)(\"applicants\")\n",
      "12. The number of applicants\n",
      "13. An abstract of the patent, where available\n",
      "14. The full text of the claims made in the patent, i.e., the patentable product or idea\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "csv_destination_path = \"data/CSVs/\"\n",
      "\n",
      "csv_source_list = [xml_destination_path + x for x in os.listdir(xml_destination_path)[1:]]\n",
      "\n",
      "# Text fields in patents can be very large, so one must set the CSV field size limit to the system maximum.\n",
      "csv.field_size_limit(sys.maxsize)\n",
      "\n",
      "# assembles all applicants' names in one string\n",
      "def applicantNames(patent):\n",
      "    return_name = ''\n",
      "    return_names = []\n",
      "    for applicants in root[patent].findall('.//applicants'):\n",
      "        apps = applicants.findall('.//applicant')\n",
      "        for applicant in apps:\n",
      "            lastname = applicant.findtext('.//addressbook/last-name') \n",
      "            firstname = applicant.findtext('.//addressbook/first-name')\n",
      "            return_name = lastname + ', ' + firstname\n",
      "            return_names.append(return_name)\n",
      "    return return_names\n",
      " \n",
      "# retrieves the patent assignee's organization name\n",
      "def assigneeNames(patent):\n",
      "    assignee_names = []\n",
      "    if not len(root[patent].findall('.//assignee')):\n",
      "        return 'None'\n",
      "    for assignee in root[patent].findall('.//assignee'):\n",
      "        name = ''.join(assignee.findtext('.//orgname'))\n",
      "        assignee_names.append(name)\n",
      "    return assignee_names\n",
      "\n",
      "# 3 functions to retrieve the patent assignee's address city, state, and country\n",
      "def assigneeCity(patent):\n",
      "    assignee_cities = []\n",
      "    if not len(root[patent].findall('.//assignee')):\n",
      "        return 'None'\n",
      "    for assignee in root[patent].findall('.//assignee'):\n",
      "        if assignee.findtext('.//city') != None:\n",
      "            for city in assignee.findall('.//city'):\n",
      "                city = ''.join(assignee.findtext('.//city').replace('\\n', ''))\n",
      "                assignee_cities.append(city)\n",
      "        else:\n",
      "            city = 'None' \n",
      "            assignee_cities.append(city)\n",
      "    return assignee_cities\n",
      "\n",
      "def assigneeState(patent):\n",
      "    assignee_states = []\n",
      "    if not len(root[patent].findall('.//assignee')):\n",
      "        return 'None'\n",
      "    for assignee in root[patent].findall('.//assignee'):\n",
      "        if assignee.findtext('.//state') != None:\n",
      "            for state in assignee.findall('.//state'):\n",
      "                state = ''.join(assignee.findtext('.//state').replace('\\n', ''))\n",
      "                assignee_states.append(state)\n",
      "        else:\n",
      "            state = 'None'\n",
      "            assignee_states.append(state)\n",
      "    return assignee_states\n",
      "\n",
      "def assigneeCountry(patent):    \n",
      "    assignee_countries = []\n",
      "    if not len(root[patent].findall('.//assignee')):\n",
      "        return 'None'\n",
      "    for assignee in root[patent].findall('.//assignee'):\n",
      "        if assignee.findtext('.//country') != None:\n",
      "            for country in assignee.findall('.//country'):\n",
      "                country = ''.join(assignee.findtext('.//country').replace('\\n', ''))\n",
      "                assignee_countries.append(country)\n",
      "        else:\n",
      "            country = 'None'\n",
      "            assignee_countries.append(country)\n",
      "    return assignee_countries\n",
      "        \n",
      "# joins all of a patent's claim text into a single string\n",
      "def claimCollect(patent):\n",
      "    return_claims = ''\n",
      "    for claim in root[patent].findall('.//claims'):\n",
      "        return_claims += ''.join(claim.itertext()).replace('\\n', ' ') + ' '\n",
      "    return return_claims\n",
      "\n",
      "# Extract all relevant patent data into one list\n",
      "def csvExtract(patent):\n",
      "    table_row = [root[patent].findtext('.//document-id/doc-number') , \\\n",
      "                 root[patent].findtext('.//invention-title'), \\\n",
      "                 root[patent].findtext('.//document-id/kind'), \\\n",
      "                 root[patent].findtext('.//document-id/date'), \\\n",
      "                 root[patent].find('.//application-reference').get('appl-type'), \\\n",
      "                 root[patent].findtext('.//application-reference//date'), \\\n",
      "                 root[patent].findtext('.//classification-national/main-classification'),\\\n",
      "                 len(root[patent].findall('.//references-cited/citation')), \\\n",
      "                 assigneeNames(patent), \\\n",
      "                 assigneeCity(patent), \\\n",
      "                 assigneeState(patent), \\\n",
      "                 assigneeCountry(patent), \\\n",
      "                 applicantNames(patent), \\\n",
      "                 len(root[patent].findall('.//applicants/applicant')), \\\n",
      "                 root[patent].findtext('.//abstract/*'), \\\n",
      "                 claimCollect(patent)]\n",
      "    return table_row \n",
      "\n",
      "# Extract desired data from formatted xmls and write to individual csvs\n",
      "# Warning: this takes a while\n",
      "# Could it be sped up with multiprocessing?\n",
      "for xml in csv_source_list[1:5]:\n",
      "    root = ET.parse(xml).getroot()\n",
      "    patent_list = root.findall('us-patent-grant')\n",
      "    patents_table = []\n",
      "    for index, patent in enumerate(patent_list):\n",
      "        try:\n",
      "            patents_table.append(csvExtract(index))\n",
      "        except:\n",
      "            continue\n",
      "    with open(csv_destination_path + xml[-13:-4] +'.csv', 'wb') as g:\n",
      "        writer = csv.writer(g)\n",
      "        writer.writerows(patents_table)\n",
      "    root.clear()  \n",
      "\n",
      "# Combine all csvs into one csv\n",
      "with open(\"/data/patents.csv\", 'wb') as g:\n",
      "    writer = csv.writer(g, delimiter = ';')\n",
      "    for csv_file in os.listdir(csv_destination_path)[1:]:\n",
      "        print 'Processing', csv_file\n",
      "        with open(csv_destination_path + csv_file, 'rU') as f:\n",
      "            reader = csv.reader(f)\n",
      "            for row in reader:  \n",
      "                writer.writerow(row)\n",
      "   "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "OSError",
       "evalue": "[Errno 2] No such file or directory: '/data/formatted_xmls/'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-10f33481cb5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcsv_destination_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/data/CSVs/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcsv_source_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxml_destination_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_destination_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Text fields in patents can be very large, so one must set the CSV field size limit to the system maximum.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: '/data/formatted_xmls/'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Cleaning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I now have data that can be read into pandas, but artifacts from the data gathering process mean there is more cleaning to be done.\n",
      "\n",
      "One issue is Unicode. The patent XMLs are encoded in UTF-8, so all data up to now has been encoded in UTF-8 as well. Python 2.5+ defaults to ASCII encoding and is quirky when it comes to working with strings encoded in UTF-8 (see [this page](https://pythonhosted.org/kitchen/unicode-frustrations.html)). \n",
      "\n",
      "Another issue is instances where a particular field, say \"Assignee Orgs,\" has multiple values. This is problematic as pandas expects each field to be a single unique value. I've dealt with this issue the best I can, but the solutions so far are imperfect.\n",
      "\n",
      "The code for further cleaning of the data is below.\n",
      "\n",
      "**Note**: It isn't necessary to run the following code. The resulting file will be provided later."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "headers = ['grant_doc_num', # assigned patent number\n",
      "'invention_title', \n",
      "'grant_kind', # type of patent granted\n",
      "'grant_date', # when patent was granted\n",
      "'appl_type', # type of patent applied for\n",
      "'appl_date', # date of patent application\n",
      "'main_class', # main US government classification of patent\n",
      "'num_refs', # number of references cited by the patent applicant and examiners\n",
      "'assignee_org', # holder of patent\n",
      "'assignee_city',    \n",
      "'assignee_state',\n",
      "'assignee_country',\n",
      "'applicant_name', # name of inventor(s)\n",
      "'num_applicants', # number of applicants\n",
      "'abstract', # synopsis of patent claims\n",
      "'claims'] # explanation of patent   \n",
      "\n",
      "date_cols = ['grant_date', 'appl_date']\n",
      "patents = pd.read_csv('data/patents.csv', \n",
      "                      sep = ';', header = None, names = headers, \n",
      "                      parse_dates = date_cols, encoding = 'utf_8')\n",
      "\n",
      "# Function to remove excess unicode characters from text fields\n",
      "def textTrans(value):\n",
      "    # Unicode value dictionary for extraneous characters \n",
      "    extraneous_table = dict.fromkeys(map(ord, '[].,\"\\'();'), None)    \n",
      "    \n",
      "    value = value.translate(extraneous_table).encode('ascii', 'ignore').lower()\n",
      "    return value\n",
      "\n",
      "# This is specifically designed to take the strings in the assignee_org column\n",
      "# and remove extraneous words. I may need to modify it to work\n",
      "# with the other text fields in the dataframe.\n",
      "# It doesn't work 100%, but it's close enough for now.\n",
      "def cleanText(value):\n",
      "    company_end_list = [u'co', u'inc', u'gmbh', u'sa', u'bv', u'ltd', \\\n",
      "                    u'corporation', u'corp', u'limited', u'company', u'lp', \\\n",
      "                    u'spa', u'mfg', u'llc', u'llp', u'as', u'ag', \\\n",
      "                    u'aktiengesellschaft', u'sas', u'kg', u'nv', u'ab', u'sl', \\\n",
      "                    u'asa', u'srl', u'ltda', u'plc', u'a/s', u'jv', u'ohg', \\\n",
      "                    u'pty', u'incorporated', u'aktiengesekkschaft', \\\n",
      "                    u'aktiengesellchaft', u'aktiengesellscahft', \\\n",
      "                    u'aktiengesellshaft', u'aktiengessellschaft', \\\n",
      "                    u'aktiengsellschaft', u'ii', u'srl', u'the', u'ulc']\n",
      "    converted_end_list = [word.encode('ascii', 'ignore') for word in \\\n",
      "    company_end_list]\n",
      "\n",
      "    if value == 'None':\n",
      "        value = np.NaN\n",
      "    else:\n",
      "        if len(value.split()) > 1:\n",
      "            elem_list = [x for x in value.split(',')]\n",
      "            name_list = []\n",
      "            for name in elem_list:\n",
      "                name = textTrans(name)\n",
      "                split_name = name.split()\n",
      "                for end in converted_end_list:\n",
      "                    if end in split_name:\n",
      "                        split_name.remove(end)\n",
      "                        name = ' '.join(split_name)\n",
      "                name_list.append(name)\n",
      "            if len(name_list) > 1 and name_list[-1] is not '':\n",
      "                return name_list\n",
      "            else:\n",
      "                return name_list[0]\n",
      "        else:\n",
      "            value = textTrans(value)\n",
      "            if ord(value[0]) == ord(' '):\n",
      "                return value.lstrip()\n",
      "            else:\n",
      "                return value\n",
      "\n",
      "# Create a new column with cleaned assignee_org names  \n",
      "patents['clean_company'] = patents.assignee_org.apply(lambda x: cleanText(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IOError",
       "evalue": "File /data/patents.csv does not exist",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-8-d60d1ff7e757>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m patents = pd.read_csv('/data/patents.csv', \n\u001b[1;32m     24\u001b[0m                       \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                       parse_dates = date_cols, encoding = 'utf_8')\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Function to remove excess unicode characters from text fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, dialect, compression, doublequote, escapechar, quotechar, quoting, skipinitialspace, lineterminator, header, index_col, names, prefix, skiprows, skipfooter, skip_footer, na_values, na_fvalues, true_values, false_values, delimiter, converters, dtype, usecols, engine, delim_whitespace, as_recarray, na_filter, compact_ints, use_unsigned, low_memory, buffer_lines, warn_bad_lines, error_bad_lines, keep_default_na, thousands, comment, decimal, parse_dates, keep_date_col, dayfirst, date_parser, memory_map, float_precision, nrows, iterator, chunksize, verbose, encoding, squeeze, mangle_dupe_cols, tupleize_cols, infer_datetime_format, skip_blank_lines)\u001b[0m\n\u001b[1;32m    463\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3150)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/Jason/anaconda/lib/python2.7/site-packages/pandas/parser.so\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:5772)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: File /data/patents.csv does not exist"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Upon reflection, I determined some of the columns I originally collected are probably unnecessary, so I removed them.\n",
      "\n",
      "The time that elapsed between the date an application was made to the time it was granted seemed like it could be important, so I calculated that.\n",
      "\n",
      "I also thought it might be interesting to get the text of the main classification headings, determined by the first three characters of main classification number in the patent data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Delete unnecessary columns\n",
      "del patents['grant_kind']\n",
      "del patents['appl_type']\n",
      "del patents['assignee_city']\n",
      "del patents['applicant_name']\n",
      "del patents['abstract']\n",
      "\n",
      "# Create a column of the length of time between the date of the patent application\n",
      "# and the date of the patent grant\n",
      "patents['time_elapsed'] = patents.grant_date - patents.appl_date\n",
      "patents['time_elapsed'].astype('timedelta64[D]')\n",
      "patents['time_elapsed'].apply(lambda x: x / np.timedelta64(1, 'D'))\n",
      "\n",
      "# Functions to identify patent main classification's first three characters and\n",
      "# extract the classification category title from the USPTO website\n",
      "r = requests.get('http://www.uspto.gov/web/patents/classification/selectnumwithtitle.htm')\n",
      "patent_soup = BeautifulSoup(r.text, 'html.parser')\n",
      "\n",
      "# Map the main classification numbers to the heading titles\n",
      "class_num = re.compile('^[\\dDGP][\\dL][\\dBT]')\n",
      "title_list = []\n",
      "for row in patent_soup.find_all('td', text = class_num):\n",
      "    class_num_txt = row.text\n",
      "    class_title = row.find_next('td').text\n",
      "    class_dict = {class_num_txt: class_title}\n",
      "    title_list.append(class_dict)\n",
      "\n",
      "# Strip the first three characters from the main classification (needs work)\n",
      "def mainClassNumber(value):\n",
      "    trunc_value_list = list(value[0:3])\n",
      "    if trunc_value_list[0] == ' ':\n",
      "        trunc_value_list.remove(' ')\n",
      "        trunc_value_list.insert(0, '0')\n",
      "    elif trunc_value_list[1] == ' ':\n",
      "        trunc_value_list.remove(' ')\n",
      "        trunc_value_list.insert(1, '0')        \n",
      "    trunc_value = ''.join(trunc_value_list)\n",
      "    return trunc_value\n",
      "\n",
      "# Gets the heading of a particular patent's classification    \n",
      "def mainClassTitle(index):\n",
      "    row_value = mainClassNumber(index)\n",
      "    for title in title_list:\n",
      "        if title.keys()[0] == row_value:\n",
      "            return title.values()[0]\n",
      "        else:\n",
      "            pass\n",
      "    \n",
      "patents['main_class_title'] = patents.main_class.apply(lambda x: mainClassTitle(x))\n",
      "\n",
      "patents.dropna(subset = ['clean_company'], inplace = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'patents' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-9-96492e5bd36f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Delete unnecessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mpatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grant_kind'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'appl_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mpatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'assignee_city'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'patents' is not defined"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "After dropping the null values of 'clean_company,' we now have a dataframe of patents that have been assigned to companies that are potentially also in the Crunchbase database. \n",
      "\n",
      "As each row in the dataframe is a patent and each patent now belongs to company, it makes sense to utilize hierarchical indexing and then perform some initial analysis.\n",
      "\n",
      "**Note:** File is provided."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Read in cleaned dataframe from csv\n",
      "date_cols = ['grant_date', 'appl_date']\n",
      "\n",
      "patents = pd.read_csv('data/patents_notnull.csv', sep = ';', parse_dates = date_cols, encoding = 'utf_8')\n",
      "\n",
      "# Reorder patent dataframe columns for readability\n",
      "patents = patents[['clean_company', 'grant_doc_num', 'invention_title',\n",
      "                   'appl_date', 'grant_date', 'time_elapsed', 'main_class', \n",
      "                   'main_class_title', 'num_refs', 'assignee_state', 'assignee_country',\n",
      "                   'num_applicants', 'claims', 'assignee_org']]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Initial Data Exploration"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.set_index(['clean_company', 'grant_doc_num'])\n",
      "patents.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "          clean_company grant_doc_num            invention_title  appl_date  \\\n",
        "0               saucony      D0651787       Articles of footwear 2010-04-30   \n",
        "1          skechers usa      D0651788                Shoe bottom 2011-07-11   \n",
        "2          skechers usa      D0651789                 Shoe upper 2011-07-11   \n",
        "3  edward a enterprises      D0651790       Tampon and pad pouch 2010-08-09   \n",
        "4            head logic      D0651791  Case for holding ear buds 2011-07-11   \n",
        "\n",
        "  grant_date                 time_elapsed main_class  \\\n",
        "0 2012-01-10  620 days 00:00:00.000000000     D 2908   \n",
        "1 2012-01-10  183 days 00:00:00.000000000     D 2954   \n",
        "2 2012-01-10  183 days 00:00:00.000000000     D 2969   \n",
        "3 2012-01-10  519 days 00:00:00.000000000    D 32035   \n",
        "4 2012-01-10  183 days 00:00:00.000000000     D 3218   \n",
        "\n",
        "                       main_class_title  num_refs assignee_state  \\\n",
        "0              Apparel and haberdashery         9         ['MA']   \n",
        "1              Apparel and haberdashery        66         ['CA']   \n",
        "2              Apparel and haberdashery        29         ['CA']   \n",
        "3  Travel goods and personal belongings        40         ['OH']   \n",
        "4  Travel goods and personal belongings        13         ['CA']   \n",
        "\n",
        "  assignee_country  num_applicants  \\\n",
        "0           ['US']               1   \n",
        "1           ['US']               1   \n",
        "2           ['US']               1   \n",
        "3           ['US']               1   \n",
        "4           ['US']               3   \n",
        "\n",
        "                                              claims  \\\n",
        "0    The ornamental design for articles of footwe...   \n",
        "1    The ornamental design for a shoe bottom, as ...   \n",
        "2    The ornamental design for a shoe upper, as s...   \n",
        "3    The ornamental design for a combined tampon ...   \n",
        "4    The ornamental design for the case for holdi...   \n",
        "\n",
        "                     assignee_org  \n",
        "0               ['Saucony, Inc.']  \n",
        "1    ['Skechers U.S.A., Inc. II']  \n",
        "2    ['SKECHERS U.S.A., Inc. II']  \n",
        "3  ['Edward A. Enterprises, LLC']  \n",
        "4             ['Head Logic, LLC']  "
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "(746435, 14)"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.clean_company.value_counts().head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "international business machines    16696\n",
        "samsung electronics                14086\n",
        "canon kabushiki kaisha              9995\n",
        "sony                                7557\n",
        "microsoft                           7362\n",
        "panasonic                           7131\n",
        "kabushiki kaisha toshiba            5697\n",
        "lg electronics                      5510\n",
        "qualcomm                            5266\n",
        "apple                               4836\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.clean_company.value_counts().tail(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "changhua christian hospital                                     1\n",
        "destech                                                         1\n",
        "ningbo dooya mechanic & electronic technology                   1\n",
        "lumisource                                                      1\n",
        "acell industries                                                1\n",
        "icoding technology                                              1\n",
        "[michelin recherche et technique,  compagnie generale des etablissement michelin]    1\n",
        "dotomi                                                          1\n",
        "phage pharmaceuticals                                           1\n",
        "[nissan motor, , nippon oil]                                    1\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.grant_doc_num.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "count       746435\n",
        "unique      746435\n",
        "top       08331271\n",
        "freq             1\n",
        "Name: grant_doc_num, dtype: object"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.time_elapsed.value_counts().head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "984 days 00:00:00.000000000     826\n",
        "966 days 00:00:00.000000000     825\n",
        "1026 days 00:00:00.000000000    816\n",
        "1089 days 00:00:00.000000000    806\n",
        "986 days 00:00:00.000000000     805\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.num_refs.describe()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "count    746435.000000\n",
        "mean         14.993181\n",
        "std          68.334154\n",
        "min           0.000000\n",
        "25%           0.000000\n",
        "50%           0.000000\n",
        "75%          10.000000\n",
        "max        3493.000000\n",
        "Name: num_refs, dtype: float64"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "patents.main_class_title.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "Drug, bio-affecting and body treating compositions              28069\n",
        "Multiplex communications                                        26213\n",
        "Surgery                                                         24438\n",
        "Active solid-state devices (e.g., transistors, solid-state diodes)    22744\n",
        "Telecommunications                                              21556\n",
        "Electrical computers and digital processing systems: multicomputer data transferring    15560\n",
        "Computer graphics processing and selective visual display systems    15327\n",
        "Semiconductor device manufacturing: process                     15301\n",
        "Data processing: database and file management or data structures    13916\n",
        "Data processing: financial, business practice, management, or cost/price determination    13808\n",
        "Image analysis                                                  13742\n",
        "Pulse or digital communications                                 12240\n",
        "Television                                                      12200\n",
        "Organic compounds -- part of the class 532-570 series           10635\n",
        "Data processing: vehicles, navigation, and relative location    10628\n",
        "...\n",
        "Leather manufactures                                          6\n",
        "Bee culture                                                   6\n",
        "Motors: spring, weight, or animal powered                     5\n",
        "Track sanders                                                 4\n",
        "Selective cutting (e.g., punching)                            4\n",
        "Tuners                                                        4\n",
        "Books, strips, and leaves for manifolding                     3\n",
        "Wooden receptacles                                            3\n",
        "Wood turning                                                  1\n",
        "Peptide or protein sequence                                   1\n",
        "Chemistry of carbon compounds                                 1\n",
        "Farriery                                                      1\n",
        "Railway wheels and axles                                      1\n",
        "Coopering                                                     1\n",
        "Electric lamp and discharge devices: consumable electrodes    1\n",
        "Length: 421, dtype: int64"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Gathering Part 2: Crunchbase"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that there is cleaned list of company names, I will use the Crunchbase API to determine which of these companies is in their database and which of those have funding data.\n",
      "\n",
      "The first step is to isolate the company names."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comps = pd.Series(patents.clean_company)\n",
      "\n",
      "indiv_comps = []\n",
      "for x in comps:\n",
      "    if type(x) is not list:\n",
      "        indiv_comps.append(x)\n",
      "        \n",
      "comp_series = pd.Series(indiv_comps)\n",
      "\n",
      "comp_series.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "(746435,)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_series.duplicated().sum()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "659550"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_series.drop_duplicates(inplace = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comp_series.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "(86885,)"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The next step is to write a wrapper for the Crunchbase API. \n",
      "\n",
      "I tried two different approaches, neither of which has worked satisfactorily. The first was to examine each individual funding round (angel, seed, Series A, etc.) for each company, gather the the date, type of funding, and the funding amount. The second approach was to only examine the total funding for each company. \n",
      "\n",
      "The files that I generated via the first and second wrappers are test_company_funds.csv and company_agg_funds.csv, respectively.\n",
      "\n",
      "The issues will be discussed in the next section."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Approach 1\n",
      "# A simple wrapper for the Crunchbase API v2 for extracting funding round data\n",
      "api_url = 'https://api.crunchbase.com/v/2/'\n",
      "api_key = '?user_key=' + api_key\n",
      "\n",
      "def fundingRound(uuid):\n",
      "    r = requests.get(api_url + 'funding-round/' + uuid + api_key)\n",
      "    round_info = r.json()\n",
      "    \n",
      "    com = round_info['data']['relationships']['funded_organization']\\\n",
      "    ['items'][0]['name'].lower()\n",
      "    date = parser.parse(round_info['data']['properties']['announced_on'], \\\n",
      "    yearfirst = True)\n",
      "    \n",
      "    try:    \n",
      "        fund_type = 'series_' + round_info['data']['properties']\\\n",
      "        ['series'].lower()\n",
      "    except:\n",
      "        fund_type = round_info['data']['properties']['funding_type']\n",
      "    \n",
      "    try:    \n",
      "        amount = round_info['data']['properties']['money_raised']\n",
      "    except:\n",
      "        amount = np.nan\n",
      "    \n",
      "    return {'company': com, 'date': date, 'fund_type': fund_type, \\\n",
      "    'amount': amount, 'uuid': uuid}\n",
      "    \n",
      "\n",
      "def cbWrap(company):\n",
      "    req = requests.get(api_url + 'organization/' + company + api_key)\n",
      "    info = req.json()\n",
      "    \n",
      "    try:        \n",
      "        rounds = [str(x['path'][14:]) for x in info['data']['relationships']\\\n",
      "        ['funding_rounds']['items']]\n",
      "        fund_rounds = [fundingRound(x) for x in rounds]\n",
      "        return fund_rounds\n",
      "            \n",
      "    except: \n",
      "        return {'company': company, 'date': np.nan, 'fund_type': np.nan, \\\n",
      "        'amount': np.nan, 'uuid': np.nan}\n",
      "    \n",
      "# Approach 2\n",
      "# A simple wrapper for the Crunchbase API v2 for extracting aggregate funding data\n",
      "def cbWrap2(company):\n",
      "    req = requests.get(api_url + 'organization/' + company + api_key)\n",
      "    info = req.json()\n",
      "    \n",
      "    try:\n",
      "        total_fund = info['data']['properties']['total_funding_usd']\n",
      "        return [company, total_fund]\n",
      "    except:\n",
      "        return [company, np.nan]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The issue with the first approach is that the returned results are incomplete. If none of the companies from the patent dataframe were in Crunchbase, one would expect that the wrapper would return each of the company names with np.nan values for the other fields. Knowing that there are companies from the patent dataframe that are in Crunchbase, it should respond with even more values. \n",
      "\n",
      "However, when I ran the code below, the wrapper worked through over 51k of the company names and returned slightly over 9k results before quitting completely."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "co_funds = []\n",
      "for x in comp_series:\n",
      "    indiv = cbWrap(x)\n",
      "    if type(indiv) is list:\n",
      "        [co_funds.append(y) for y in indiv]\n",
      "    else: \n",
      "        co_funds.append(indiv)\n",
      "    time.sleep(.7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The issue with the second approach is that the wrapper only worked for slightly over 3600 entries, again before quitting completely. \n",
      "\n",
      "This is odd because Crunchbase has granted me API access for 125,000 hits per day / 100 per minute, which I would think would be ample. \n",
      "\n",
      "Nonetheless, the code for running the second approach is below."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "co_funds_agg = []\n",
      "\n",
      "for x in comp_series.iloc[3629:]:\n",
      "    co_funds_agg.append(cbWrap(x))\n",
      "    time.sleep(.8)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Load the results from the semi-successful wrappers\n",
      "com_funds_agg = pd.read_csv('data/company_agg_funds.csv', encoding = 'utf_8', sep = ';')\n",
      "com_funds_gran = pd.read_csv('data/test_company_funds.csv', encoding = 'utf_8', sep = ';')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "com_funds_agg.columns = ['company', 'total_funds']\n",
      "com_funds_agg.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "(1255, 2)"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "com_funds_gran.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "(1748, 5)"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "com_funds_agg.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "(1255, 2)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "com_funds_agg.hist(bins = 5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x11168b790>]], dtype=object)"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEXCAYAAACkpJNEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wCgWH6ywYF/dgAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x1116a4690>"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "com_funds_gran.fund_type.value_counts()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "venture                 501\n",
        "debt_financing          226\n",
        "series_a                185\n",
        "series_b                175\n",
        "series_c                146\n",
        "seed                    144\n",
        "series_d                 85\n",
        "private_equity           72\n",
        "grant                    51\n",
        "series_e                 37\n",
        "post_ipo_equity          30\n",
        "angel                    22\n",
        "undisclosed              22\n",
        "series_f                 16\n",
        "convertible_note         12\n",
        "product_crowdfunding      9\n",
        "post_ipo_debt             6\n",
        "equity_crowdfunding       4\n",
        "series_g                  4\n",
        "secondary_market          1\n",
        "dtype: int64"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Data Analysis"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In short, this is as far as gotten having spent so much time on data acquisition and cleaning.\n",
      "\n",
      "What I hope to do once all of the data issues are figured out includes:\n",
      "1. Linear regression between number of patents and total funding amount\n",
      "2. Categorization of patent claim texts (either LDA or TFIDF) with type of funding (venture, seed, etc.)\n",
      "3. Maybe see if there's a relationship between patent claim texts and amount of funding?\n",
      "4. Much better visuals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}